{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.15192950470981464,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020257267294641955,
      "grad_norm": 2.4405038356781006,
      "learning_rate": 4.998075559607009e-05,
      "loss": 0.5615,
      "step": 20
    },
    {
      "epoch": 0.004051453458928391,
      "grad_norm": 1.4659157991409302,
      "learning_rate": 4.996049832877545e-05,
      "loss": 0.3374,
      "step": 40
    },
    {
      "epoch": 0.006077180188392586,
      "grad_norm": 1.5483160018920898,
      "learning_rate": 4.994024106148081e-05,
      "loss": 0.2433,
      "step": 60
    },
    {
      "epoch": 0.008102906917856782,
      "grad_norm": 2.1053307056427,
      "learning_rate": 4.9919983794186165e-05,
      "loss": 0.2178,
      "step": 80
    },
    {
      "epoch": 0.010128633647320976,
      "grad_norm": 0.9805480241775513,
      "learning_rate": 4.989972652689153e-05,
      "loss": 0.2048,
      "step": 100
    },
    {
      "epoch": 0.012154360376785172,
      "grad_norm": 2.8173420429229736,
      "learning_rate": 4.987946925959689e-05,
      "loss": 0.1792,
      "step": 120
    },
    {
      "epoch": 0.014180087106249366,
      "grad_norm": 3.2427754402160645,
      "learning_rate": 4.985921199230224e-05,
      "loss": 0.1567,
      "step": 140
    },
    {
      "epoch": 0.016205813835713564,
      "grad_norm": 1.0643627643585205,
      "learning_rate": 4.9838954725007595e-05,
      "loss": 0.1468,
      "step": 160
    },
    {
      "epoch": 0.018231540565177756,
      "grad_norm": 2.5849173069000244,
      "learning_rate": 4.981971032107769e-05,
      "loss": 0.1942,
      "step": 180
    },
    {
      "epoch": 0.020257267294641952,
      "grad_norm": 13.793347358703613,
      "learning_rate": 4.979945305378305e-05,
      "loss": 0.1739,
      "step": 200
    },
    {
      "epoch": 0.022282994024106148,
      "grad_norm": 1.154039978981018,
      "learning_rate": 4.97791957864884e-05,
      "loss": 0.1475,
      "step": 220
    },
    {
      "epoch": 0.024308720753570344,
      "grad_norm": 1.3792349100112915,
      "learning_rate": 4.9758938519193764e-05,
      "loss": 0.1279,
      "step": 240
    },
    {
      "epoch": 0.02633444748303454,
      "grad_norm": 1.2435647249221802,
      "learning_rate": 4.973868125189912e-05,
      "loss": 0.1234,
      "step": 260
    },
    {
      "epoch": 0.028360174212498732,
      "grad_norm": 0.7834601402282715,
      "learning_rate": 4.971842398460448e-05,
      "loss": 0.1422,
      "step": 280
    },
    {
      "epoch": 0.030385900941962928,
      "grad_norm": 0.8932020664215088,
      "learning_rate": 4.9698166717309836e-05,
      "loss": 0.1231,
      "step": 300
    },
    {
      "epoch": 0.03241162767142713,
      "grad_norm": 1.0319157838821411,
      "learning_rate": 4.96779094500152e-05,
      "loss": 0.1448,
      "step": 320
    },
    {
      "epoch": 0.03443735440089132,
      "grad_norm": 0.9127151966094971,
      "learning_rate": 4.965765218272056e-05,
      "loss": 0.1259,
      "step": 340
    },
    {
      "epoch": 0.03646308113035551,
      "grad_norm": 1.1230705976486206,
      "learning_rate": 4.963739491542591e-05,
      "loss": 0.1174,
      "step": 360
    },
    {
      "epoch": 0.03848880785981971,
      "grad_norm": 4.197830677032471,
      "learning_rate": 4.9617137648131265e-05,
      "loss": 0.1239,
      "step": 380
    },
    {
      "epoch": 0.040514534589283904,
      "grad_norm": 0.9139384627342224,
      "learning_rate": 4.959688038083663e-05,
      "loss": 0.1511,
      "step": 400
    },
    {
      "epoch": 0.0425402613187481,
      "grad_norm": 1.3449727296829224,
      "learning_rate": 4.957662311354199e-05,
      "loss": 0.1322,
      "step": 420
    },
    {
      "epoch": 0.044565988048212296,
      "grad_norm": 3.993757724761963,
      "learning_rate": 4.9556365846247344e-05,
      "loss": 0.1058,
      "step": 440
    },
    {
      "epoch": 0.04659171477767649,
      "grad_norm": 4.853125095367432,
      "learning_rate": 4.95361085789527e-05,
      "loss": 0.1022,
      "step": 460
    },
    {
      "epoch": 0.04861744150714069,
      "grad_norm": 1.7318204641342163,
      "learning_rate": 4.9515851311658066e-05,
      "loss": 0.126,
      "step": 480
    },
    {
      "epoch": 0.050643168236604884,
      "grad_norm": 2.970012664794922,
      "learning_rate": 4.9495594044363416e-05,
      "loss": 0.1009,
      "step": 500
    },
    {
      "epoch": 0.05266889496606908,
      "grad_norm": 2.0667383670806885,
      "learning_rate": 4.9475336777068773e-05,
      "loss": 0.1033,
      "step": 520
    },
    {
      "epoch": 0.054694621695533276,
      "grad_norm": 1.1282299757003784,
      "learning_rate": 4.945507950977413e-05,
      "loss": 0.0934,
      "step": 540
    },
    {
      "epoch": 0.056720348424997465,
      "grad_norm": 2.0177979469299316,
      "learning_rate": 4.943482224247949e-05,
      "loss": 0.1238,
      "step": 560
    },
    {
      "epoch": 0.05874607515446166,
      "grad_norm": 1.2376540899276733,
      "learning_rate": 4.941456497518485e-05,
      "loss": 0.0871,
      "step": 580
    },
    {
      "epoch": 0.060771801883925856,
      "grad_norm": 2.9011895656585693,
      "learning_rate": 4.939430770789021e-05,
      "loss": 0.1128,
      "step": 600
    },
    {
      "epoch": 0.06279752861339005,
      "grad_norm": 1.1684095859527588,
      "learning_rate": 4.937405044059557e-05,
      "loss": 0.1189,
      "step": 620
    },
    {
      "epoch": 0.06482325534285426,
      "grad_norm": 0.638354480266571,
      "learning_rate": 4.9353793173300924e-05,
      "loss": 0.1053,
      "step": 640
    },
    {
      "epoch": 0.06684898207231844,
      "grad_norm": 2.9932680130004883,
      "learning_rate": 4.933353590600628e-05,
      "loss": 0.085,
      "step": 660
    },
    {
      "epoch": 0.06887470880178263,
      "grad_norm": 8.7505464553833,
      "learning_rate": 4.931327863871164e-05,
      "loss": 0.1089,
      "step": 680
    },
    {
      "epoch": 0.07090043553124684,
      "grad_norm": 2.4463045597076416,
      "learning_rate": 4.9293021371416996e-05,
      "loss": 0.0881,
      "step": 700
    },
    {
      "epoch": 0.07292616226071102,
      "grad_norm": 2.3883886337280273,
      "learning_rate": 4.9272764104122354e-05,
      "loss": 0.1098,
      "step": 720
    },
    {
      "epoch": 0.07495188899017523,
      "grad_norm": 1.602027177810669,
      "learning_rate": 4.925250683682772e-05,
      "loss": 0.1225,
      "step": 740
    },
    {
      "epoch": 0.07697761571963942,
      "grad_norm": 3.593160390853882,
      "learning_rate": 4.9232249569533075e-05,
      "loss": 0.1089,
      "step": 760
    },
    {
      "epoch": 0.07900334244910362,
      "grad_norm": 0.7113755345344543,
      "learning_rate": 4.921199230223843e-05,
      "loss": 0.0966,
      "step": 780
    },
    {
      "epoch": 0.08102906917856781,
      "grad_norm": 3.9807815551757812,
      "learning_rate": 4.919173503494379e-05,
      "loss": 0.1001,
      "step": 800
    },
    {
      "epoch": 0.08305479590803201,
      "grad_norm": 1.8223150968551636,
      "learning_rate": 4.917147776764915e-05,
      "loss": 0.1339,
      "step": 820
    },
    {
      "epoch": 0.0850805226374962,
      "grad_norm": 2.0231804847717285,
      "learning_rate": 4.9151220500354504e-05,
      "loss": 0.1171,
      "step": 840
    },
    {
      "epoch": 0.0871062493669604,
      "grad_norm": 2.085484027862549,
      "learning_rate": 4.913096323305986e-05,
      "loss": 0.1049,
      "step": 860
    },
    {
      "epoch": 0.08913197609642459,
      "grad_norm": 2.450080633163452,
      "learning_rate": 4.911070596576522e-05,
      "loss": 0.0904,
      "step": 880
    },
    {
      "epoch": 0.09115770282588878,
      "grad_norm": 3.7373743057250977,
      "learning_rate": 4.909044869847058e-05,
      "loss": 0.1062,
      "step": 900
    },
    {
      "epoch": 0.09318342955535298,
      "grad_norm": 0.4202941656112671,
      "learning_rate": 4.907019143117594e-05,
      "loss": 0.1187,
      "step": 920
    },
    {
      "epoch": 0.09520915628481717,
      "grad_norm": 1.1584241390228271,
      "learning_rate": 4.90499341638813e-05,
      "loss": 0.1048,
      "step": 940
    },
    {
      "epoch": 0.09723488301428138,
      "grad_norm": 5.440037250518799,
      "learning_rate": 4.902967689658665e-05,
      "loss": 0.1098,
      "step": 960
    },
    {
      "epoch": 0.09926060974374556,
      "grad_norm": 4.4612321853637695,
      "learning_rate": 4.9009419629292006e-05,
      "loss": 0.1114,
      "step": 980
    },
    {
      "epoch": 0.10128633647320977,
      "grad_norm": 0.742353618144989,
      "learning_rate": 4.898916236199737e-05,
      "loss": 0.0886,
      "step": 1000
    },
    {
      "epoch": 0.10331206320267396,
      "grad_norm": 1.4377676248550415,
      "learning_rate": 4.896890509470273e-05,
      "loss": 0.0793,
      "step": 1020
    },
    {
      "epoch": 0.10533778993213816,
      "grad_norm": 4.384881973266602,
      "learning_rate": 4.8948647827408084e-05,
      "loss": 0.0849,
      "step": 1040
    },
    {
      "epoch": 0.10736351666160235,
      "grad_norm": 4.020693778991699,
      "learning_rate": 4.892839056011344e-05,
      "loss": 0.0966,
      "step": 1060
    },
    {
      "epoch": 0.10938924339106655,
      "grad_norm": 3.715514659881592,
      "learning_rate": 4.8908133292818806e-05,
      "loss": 0.0908,
      "step": 1080
    },
    {
      "epoch": 0.11141497012053074,
      "grad_norm": 3.8350300788879395,
      "learning_rate": 4.8887876025524156e-05,
      "loss": 0.0788,
      "step": 1100
    },
    {
      "epoch": 0.11344069684999493,
      "grad_norm": 3.954622983932495,
      "learning_rate": 4.8867618758229514e-05,
      "loss": 0.1081,
      "step": 1120
    },
    {
      "epoch": 0.11546642357945913,
      "grad_norm": 2.855602264404297,
      "learning_rate": 4.884736149093487e-05,
      "loss": 0.0886,
      "step": 1140
    },
    {
      "epoch": 0.11749215030892332,
      "grad_norm": 1.824524998664856,
      "learning_rate": 4.8827104223640235e-05,
      "loss": 0.1221,
      "step": 1160
    },
    {
      "epoch": 0.11951787703838752,
      "grad_norm": 1.3839136362075806,
      "learning_rate": 4.880684695634559e-05,
      "loss": 0.0919,
      "step": 1180
    },
    {
      "epoch": 0.12154360376785171,
      "grad_norm": 0.7302467823028564,
      "learning_rate": 4.878658968905095e-05,
      "loss": 0.0912,
      "step": 1200
    },
    {
      "epoch": 0.12356933049731592,
      "grad_norm": 3.86795711517334,
      "learning_rate": 4.876633242175631e-05,
      "loss": 0.0926,
      "step": 1220
    },
    {
      "epoch": 0.1255950572267801,
      "grad_norm": 1.6369811296463013,
      "learning_rate": 4.8746075154461664e-05,
      "loss": 0.1145,
      "step": 1240
    },
    {
      "epoch": 0.1276207839562443,
      "grad_norm": 3.724612236022949,
      "learning_rate": 4.872581788716702e-05,
      "loss": 0.0853,
      "step": 1260
    },
    {
      "epoch": 0.1296465106857085,
      "grad_norm": 1.9994053840637207,
      "learning_rate": 4.870556061987238e-05,
      "loss": 0.1069,
      "step": 1280
    },
    {
      "epoch": 0.1316722374151727,
      "grad_norm": 3.5466084480285645,
      "learning_rate": 4.8685303352577736e-05,
      "loss": 0.0942,
      "step": 1300
    },
    {
      "epoch": 0.1336979641446369,
      "grad_norm": 2.0595154762268066,
      "learning_rate": 4.86650460852831e-05,
      "loss": 0.1033,
      "step": 1320
    },
    {
      "epoch": 0.13572369087410108,
      "grad_norm": 4.374864101409912,
      "learning_rate": 4.864478881798846e-05,
      "loss": 0.1002,
      "step": 1340
    },
    {
      "epoch": 0.13774941760356527,
      "grad_norm": 1.7625739574432373,
      "learning_rate": 4.8624531550693815e-05,
      "loss": 0.0841,
      "step": 1360
    },
    {
      "epoch": 0.13977514433302948,
      "grad_norm": 0.4529566764831543,
      "learning_rate": 4.860427428339917e-05,
      "loss": 0.0925,
      "step": 1380
    },
    {
      "epoch": 0.14180087106249367,
      "grad_norm": 0.9075517654418945,
      "learning_rate": 4.858401701610453e-05,
      "loss": 0.0806,
      "step": 1400
    },
    {
      "epoch": 0.14382659779195786,
      "grad_norm": 2.2661941051483154,
      "learning_rate": 4.856375974880989e-05,
      "loss": 0.0981,
      "step": 1420
    },
    {
      "epoch": 0.14585232452142205,
      "grad_norm": 1.4321025609970093,
      "learning_rate": 4.8543502481515245e-05,
      "loss": 0.0979,
      "step": 1440
    },
    {
      "epoch": 0.14787805125088627,
      "grad_norm": 1.6863058805465698,
      "learning_rate": 4.85232452142206e-05,
      "loss": 0.0853,
      "step": 1460
    },
    {
      "epoch": 0.14990377798035046,
      "grad_norm": 5.636297702789307,
      "learning_rate": 4.850298794692596e-05,
      "loss": 0.0824,
      "step": 1480
    },
    {
      "epoch": 0.15192950470981464,
      "grad_norm": 3.084430694580078,
      "learning_rate": 4.848273067963132e-05,
      "loss": 0.0846,
      "step": 1500
    }
  ],
  "logging_steps": 20,
  "max_steps": 49365,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.05167733129216e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
